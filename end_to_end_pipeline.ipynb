{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50d700f1",
   "metadata": {},
   "source": [
    "# Data Ingestion \n",
    "\n",
    "- ###  Using Wikipedia of \"volcanos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a3f29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from bs4 import BeautifulSoup,SoupStrainer\n",
    "loader = WebBaseLoader(\n",
    "    web_path=\"https://en.wikipedia.org/wiki/Volcano\",\n",
    "    bs_kwargs={\n",
    "        \"parse_only\": SoupStrainer(\n",
    "            class_=[\"mw-content-ltr mw-parser-output\"]    ## Scrapping Only the main content \n",
    "        )\n",
    "    }\n",
    ")\n",
    "loader.requests_kwargs = {'verify':False}   # disable SSL certificate verification\n",
    "raw_docs=loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d47314a",
   "metadata": {},
   "source": [
    "## Cleaning noise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da529fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "raw_text=raw_docs[0].page_content\n",
    "def clean_wikipedia_text(text):\n",
    "    \n",
    "    # 1. Removing citation brackets like [1], [24]\n",
    "    text = re.sub(r'\\[\\d+\\]', '', text)\n",
    "    \n",
    "    # 2. Removinf [edit] text from headers\n",
    "    text = re.sub(r'\\[edit\\]', '', text)\n",
    "    \n",
    "    # 3. Standardize whitespace (replace multiple newlines with one)\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "    \n",
    "    # 4. Remove leading/trailing whitespace\n",
    "    return text.strip()\n",
    "\n",
    "cleaned_text = clean_wikipedia_text(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c710261a",
   "metadata": {},
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f75b1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=80,\n",
    "    # length_function=len,\n",
    "    # separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"] # Spliting by Paragraphs first, then Sentences\n",
    ")\n",
    "chunks = text_splitter.split_text(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b88c4a0",
   "metadata": {},
   "source": [
    "# Metadata Enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d59e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "enriched_docs = [\n",
    "    Document(\n",
    "        page_content=chunk, \n",
    "        metadata={\"source\": \"wikipedia\", \"topic\": \"volcano\", \"chunk_id\": i}\n",
    "    ) \n",
    "    for i, chunk in enumerate(chunks)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e5755f",
   "metadata": {},
   "source": [
    "## Viewing Cleaned Chunk Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba99afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"We get {len(enriched_docs)} documents with metadata.\")\n",
    "print(f\"Example Metadata: {enriched_docs[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5e70da",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfb0676",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf296065",
   "metadata": {},
   "source": [
    "# Vector_Db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958cfff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vector_db = FAISS.from_documents(enriched_docs, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a588f6",
   "metadata": {},
   "source": [
    "## Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7c2c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "from langchain_community.document_compressors import FlashrankRerank\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "base_retriever = vector_db.as_retriever(search_kwargs={\"k\": 10})\n",
    "\n",
    "# Initializing the FlashRank Reranker -->AI using\n",
    "compressor = FlashrankRerank()\n",
    "\n",
    "# Creating the Compression Retriever\n",
    "rerank_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, \n",
    "    base_retriever=base_retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0767b8a6",
   "metadata": {},
   "source": [
    "## Using Open_AI API KEY for Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d7c1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found. Check your .env file!\")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9ca6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.chains import create_retrieval_chain\n",
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0) # gpt-4o is excellent for RAG\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an expert assistant for geologic research. \"\n",
    "    \"Use the provided context to answer the question concisely.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "combine_docs_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(rerank_retriever, combine_docs_chain)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24783514",
   "metadata": {},
   "source": [
    "# Testing Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eebe1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag_chain.invoke({\"input\": \"What are the long-term environmental impacts of volcanic eruptions?\"})\n",
    "\n",
    "print(f\"Answer: {response['answer']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG (3.13.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
